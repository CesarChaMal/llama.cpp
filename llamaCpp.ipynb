{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CesarChaMal/llamaCpp/blob/master/llamaCpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3L8Lkyj-5gl"
      },
      "source": [
        "#Clone repo, download model and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aHr4Fo-7IRy"
      },
      "outputs": [],
      "source": [
        "!rm -rf *\n",
        "!git lfs install\n",
        "!sudo apt update\n",
        "!sudo apt install -y libimagequant-dev\n",
        "!sudo apt install -y make\n",
        "!sudo apt install -y cmake\n",
        "!git clone https://github.com/CesarChaMal/llamaCpp.git\n",
        "!mv llamaCpp/* .\n",
        "!rm -rf llamaCpp/\n",
        "\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!source /usr/local/etc/profile.d/conda.sh && conda init bash\n",
        "!source ~/.bashrc\n",
        "!conda create --name venv python=3.8 -y\n",
        "!source /usr/local/etc/profile.d/conda.sh && conda activate venv\n",
        "!/usr/local/envs/venv/bin/python --version\n",
        "!python -c \"import os; print('Current Working Directory:', os.getcwd())\"\n",
        "!/usr/local/envs/venv/bin/python -c \"import os; print('Current Working Directory:', os.getcwd())\"\n",
        "\n",
        "!/usr/local/envs/venv/bin/python -m pip install -r /content/requirements.txt\n",
        "\n",
        "!cmake ..\n",
        "!make\n",
        "\n",
        "#!git clone https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B openHermes-7b-v2.5\n",
        "#!mv openHermes-7b-v2.5 models/\n",
        "!git clone https://huggingface.co/teknium/OpenHermes-7B openHermes-7b\n",
        "!mv openHermes-7b models/\n",
        "\n",
        "#!/usr/local/envs/venv/bin/python /content/convert.py /content/models/openHermes-7b-v2.5 --outfile /content/models/openHermes-7b-v2.5/ggml-model-f16.gguf --outtype f16\n",
        "!/usr/local/envs/venv/bin/python convert.py /content/models/openHermes-7b --outfile /content/models/openHermes-7b/ggml-model-f16.gguf --outtype f16\n",
        "\n",
        "!/usr/local/envs/venv/bin/python -m pip install sentencepiece\n",
        "\n",
        "#./quantize /content/models/openHermes-7b-v2..5/ggml-model-f16.gguf /content/models/openHermes-7b-v2..5/ggml-model-q8_0.gguf q8_0\n",
        "!./quantize /content/models/openHermes-7b/ggml-model-f16.gguf /content/models/openHermes-7b/ggml-model-q8_0.gguf q8_0\n",
        "\n",
        "#./quantize /content/models/openHermes-7b-v2..5/ggml-model-f16.gguf /content/models/openHermes-7b-v2..5/ggml-model-q4_k.gguf q4_k\n",
        "!./quantize /content/models/openHermes-7b/ggml-model-f16.gguf /content/models/openHermes-7b/ggml-model-q4_k.gguf q4_k\n",
        "\n",
        "#!./batched-bench /content/models/openHermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 2048 128,512 1,2,3,4\n",
        "#!./batched-bench /content/models/openHermes-7b/ggml-model-f16.gguf 4096 0 99 0 2048 128,512 1,2,3,4\n",
        "\n",
        "#!./server -m /content/models/openHermes-7b-v2.5/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512\n",
        "#!./server -m /content/models/openHermes-7b/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512\n",
        "\n",
        "#!nohup ./server -m /content/models/openHermes-7b-v2.5/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512 &\n",
        "!nohup ./server -m /content/models/openHermes-7b/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512 &\n",
        "\n",
        "!/usr/local/envs/venv/bin/python -m pip install pyngrok\n",
        "!/usr/local/envs/venv/bin/python -m pip show pyngrok\n",
        "\n",
        "!/usr/local/envs/venv/bin/python -c \"from pyngrok import ngrok; ngrok.set_auth_token('mytoken'); public_url = ngrok.connect(8888, 'http'); print('Ngrok Tunnel URL:', public_url)\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}