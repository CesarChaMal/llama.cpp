{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CesarChaMal/llamaCpp/blob/master/llamaCpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3L8Lkyj-5gl"
      },
      "source": [
        "#Clone repo, download model and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0aHr4Fo-7IRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c823735-4e18-459d-facb-0e1e30268c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [645 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,595 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,617 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,049 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,340 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,631 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,309 kB]\n",
            "Fetched 9,421 kB in 4s (2,184 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "24 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libimagequant0\n",
            "The following NEW packages will be installed:\n",
            "  libimagequant-dev libimagequant0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 68.9 kB of archives.\n",
            "After this operation, 191 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant0 amd64 2.17.0-1 [34.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant-dev amd64 2.17.0-1 [34.3 kB]\n",
            "Fetched 68.9 kB in 0s (141 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libimagequant0:amd64.\n",
            "(Reading database ... 121654 files and directories currently installed.)\n",
            "Preparing to unpack .../libimagequant0_2.17.0-1_amd64.deb ...\n",
            "Unpacking libimagequant0:amd64 (2.17.0-1) ...\n",
            "Selecting previously unselected package libimagequant-dev:amd64.\n",
            "Preparing to unpack .../libimagequant-dev_2.17.0-1_amd64.deb ...\n",
            "Unpacking libimagequant-dev:amd64 (2.17.0-1) ...\n",
            "Setting up libimagequant0:amd64 (2.17.0-1) ...\n",
            "Setting up libimagequant-dev:amd64 (2.17.0-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "make is already the newest version (4.3-4.1build1).\n",
            "make set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Cloning into 'llamaCpp'...\n",
            "remote: Enumerating objects: 15101, done.\u001b[K\n",
            "remote: Counting objects: 100% (5667/5667), done.\u001b[K\n",
            "remote: Compressing objects: 100% (251/251), done.\u001b[K\n",
            "remote: Total 15101 (delta 5529), reused 5429 (delta 5416), pack-reused 9434\u001b[K\n",
            "Receiving objects: 100% (15101/15101), 17.54 MiB | 17.12 MiB/s, done.\n",
            "Resolving deltas: 100% (10550/10550), done.\n",
            "Cloning into 'openHermes-7b'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Total 28 (delta 0), reused 0 (delta 0), pack-reused 28\u001b[K\n",
            "Unpacking objects: 100% (28/28), 6.32 KiB | 588.00 KiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.55 GiB | 7.28 MiB/s, done.\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00001-of-00002.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n",
            "--2024-01-12 08:00:12--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 141613749 (135M) [text/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>] 135.05M   103MB/s    in 1.3s    \n",
            "\n",
            "2024-01-12 08:00:13 (103 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [141613749/141613749]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "                                                                                       \n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "no change     /usr/local/condabin/conda\n",
            "no change     /usr/local/bin/conda\n",
            "no change     /usr/local/bin/conda-env\n",
            "no change     /usr/local/bin/activate\n",
            "no change     /usr/local/bin/deactivate\n",
            "no change     /usr/local/etc/profile.d/conda.sh\n",
            "no change     /usr/local/etc/fish/conf.d/conda.fish\n",
            "no change     /usr/local/shell/condabin/Conda.psm1\n",
            "no change     /usr/local/shell/condabin/conda-hook.ps1\n",
            "no change     /usr/local/lib/python3.11/site-packages/xontrib/conda.xsh\n",
            "no change     /usr/local/etc/profile.d/conda.csh\n",
            "modified      /root/.bashrc\n",
            "\n",
            "==> For changes to take effect, close and re-open your current shell. <==\n",
            "\n",
            "Channels:\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/venv\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    pip-23.3.1                 |   py38h06a4308_0         2.6 MB\n",
            "    python-3.8.18              |       h955ad1f_0        25.3 MB\n",
            "    setuptools-68.2.2          |   py38h06a4308_0         948 KB\n",
            "    wheel-0.41.2               |   py38h06a4308_0         108 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        28.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2023.12.12-h06a4308_0 \n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1 \n",
            "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_0 \n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
            "  openssl            pkgs/main/linux-64::openssl-3.0.12-h7f8727e_0 \n",
            "  pip                pkgs/main/linux-64::pip-23.3.1-py38h06a4308_0 \n",
            "  python             pkgs/main/linux-64::python-3.8.18-h955ad1f_0 \n",
            "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
            "  setuptools         pkgs/main/linux-64::setuptools-68.2.2-py38h06a4308_0 \n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.41.2-h5eee18b_0 \n",
            "  tk                 pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0 \n",
            "  wheel              pkgs/main/linux-64::wheel-0.41.2-py38h06a4308_0 \n",
            "  xz                 pkgs/main/linux-64::xz-5.4.5-h5eee18b_0 \n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_0 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.8.18        | 25.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "pip-23.3.1           | 2.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "setuptools-68.2.2    | 948 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "wheel-0.41.2         | 108 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "pip-23.3.1           | 2.6 MB    | :   1% 0.00598361080818919/1 [00:00<00:44, 44.61s/it]\u001b[A\n",
            "\n",
            "setuptools-68.2.2    | 948 KB    | :   2% 0.01688591699681328/1 [00:00<00:15, 15.91s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "python-3.8.18        | 25.3 MB   | :  13% 0.12866579134397071/1 [00:00<00:02,  2.67s/it]   \n",
            "\n",
            "\n",
            "wheel-0.41.2         | 108 KB    | : 100% 1.0/1 [00:00<00:00,  2.45it/s]               \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "python-3.8.18        | 25.3 MB   | :  73% 0.7299309316629108/1 [00:00<00:00,  1.36it/s]\n",
            "\n",
            "setuptools-68.2.2    | 948 KB    | : 100% 1.0/1 [00:01<00:00,  1.16s/it]                \u001b[A\u001b[A\n",
            "\n",
            "setuptools-68.2.2    | 948 KB    | : 100% 1.0/1 [00:01<00:00,  1.16s/it]\u001b[A\u001b[A\n",
            "pip-23.3.1           | 2.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.90s/it]                \u001b[A\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate venv\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Python 3.8.18\n",
            "Current Working Directory: /content\n",
            "Current Working Directory: /content\n",
            "Collecting numpy~=1.24.4 (from -r /content/./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting sentencepiece~=0.1.98 (from -r /content/./requirements/requirements-convert.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.35.2 (from -r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gguf>=0.1.0 (from -r /content/./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r /content/./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting torch~=2.1.1 (from -r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting filelock (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting packaging>=20.0 (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading tokenizers-0.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading safetensors-0.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting fsspec (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.1.0 (from torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers<5.0.0,>=4.35.2->-r /content/./requirements/requirements-convert.txt (line 3))\n",
            "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting mpmath>=0.19 (from sympy->torch~=2.1.1->-r /content/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.2-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.0/777.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, mpmath, urllib3, typing-extensions, tqdm, sympy, safetensors, regex, pyyaml, protobuf, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, gguf, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers\n",
            "Successfully installed MarkupSafe-2.1.3 certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.12.2 gguf-0.6.0 huggingface-hub-0.20.2 idna-3.6 jinja2-3.1.3 mpmath-1.3.0 networkx-3.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 packaging-23.2 protobuf-4.25.2 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.1 sentencepiece-0.1.99 sympy-1.12 tokenizers-0.15.0 torch-2.1.2 tqdm-4.66.1 transformers-4.36.2 triton-2.1.0 typing-extensions-4.9.0 urllib3-2.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mCMake Warning:\n",
            "  Ignoring extra path from command line:\n",
            "\n",
            "   \"..\"\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[0mCMake Error: The source directory \"/\" does not appear to contain CMakeLists.txt.\n",
            "Specify --help for usage, or press the help button on the CMake GUI.\u001b[0m\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server    -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead  \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Loading model file models/openHermes-7b/pytorch_model-00001-of-00002.bin\n",
            "Loading model file models/openHermes-7b/pytorch_model-00001-of-00002.bin\n",
            "Loading model file models/openHermes-7b/pytorch_model-00002-of-00002.bin\n",
            "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('models/openHermes-7b'))\n",
            "32000 32000\n",
            "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'pad': 0}, add special tokens {'bos': True, 'eos': False}>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | F16    | [4096]\n",
            "lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]\n",
            "Writing models/openHermes-7b/ggml-model-f16.gguf, format 1\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: WARNING: Adding merges requested but no merges found, output may be non-functional.\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type pad to 0\n",
            "gguf: Setting add_bos_token to True\n",
            "gguf: Setting add_eos_token to False\n",
            "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   2\n",
            "[  2/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
            "[  3/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
            "[  4/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   5\n",
            "[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5\n",
            "[  6/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+   5\n",
            "[  7/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+   5\n",
            "[  8/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+   6\n",
            "[  9/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   6\n",
            "[ 10/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   6\n",
            "[ 11/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   6\n",
            "[ 12/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type F16  | T+   6\n",
            "[ 13/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type F16  | T+   6\n",
            "[ 14/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+  10\n",
            "[ 15/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  10\n",
            "[ 16/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  10\n",
            "[ 17/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  11\n",
            "[ 18/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  11\n",
            "[ 19/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  11\n",
            "[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
            "[ 21/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
            "[ 22/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
            "[ 23/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+  12\n",
            "[ 24/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  13\n",
            "[ 25/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  16\n",
            "[ 26/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  16\n",
            "[ 27/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  16\n",
            "[ 28/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  16\n",
            "[ 29/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  16\n",
            "[ 30/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  16\n",
            "[ 31/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  16\n",
            "[ 32/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  16\n",
            "[ 33/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  17\n",
            "[ 34/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  18\n",
            "[ 35/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  19\n",
            "[ 36/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  20\n",
            "[ 37/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  20\n",
            "[ 38/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  20\n",
            "[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  21\n",
            "[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  21\n",
            "[ 41/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  21\n",
            "[ 42/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  22\n",
            "[ 43/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  25\n",
            "[ 44/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  26\n",
            "[ 45/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  30\n",
            "[ 46/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  30\n",
            "[ 47/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  30\n",
            "[ 48/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  30\n",
            "[ 49/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  30\n",
            "[ 50/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  30\n",
            "[ 51/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  30\n",
            "[ 52/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  31\n",
            "[ 53/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  32\n",
            "[ 54/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  35\n",
            "[ 55/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  35\n",
            "[ 56/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  35\n",
            "[ 57/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  36\n",
            "[ 58/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  36\n",
            "[ 59/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  36\n",
            "[ 60/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  36\n",
            "[ 61/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  40\n",
            "[ 62/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  41\n",
            "[ 63/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  41\n",
            "[ 64/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  41\n",
            "[ 65/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  41\n",
            "[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  41\n",
            "[ 67/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  42\n",
            "[ 68/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  45\n",
            "[ 69/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  45\n",
            "[ 70/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  45\n",
            "[ 71/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  46\n",
            "[ 72/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  46\n",
            "[ 73/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  46\n",
            "[ 74/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  46\n",
            "[ 75/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  46\n",
            "[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  46\n",
            "[ 77/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  47\n",
            "[ 78/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  48\n",
            "[ 79/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  49\n",
            "[ 80/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  50\n",
            "[ 81/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  51\n",
            "[ 82/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  51\n",
            "[ 83/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  51\n",
            "[ 84/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type F16  | T+  51\n",
            "[ 85/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type F16  | T+  51\n",
            "[ 86/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  52\n",
            "[ 87/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type F16  | T+  52\n",
            "[ 88/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type F16  | T+  53\n",
            "[ 89/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type F16  | T+  54\n",
            "[ 90/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  55\n",
            "[ 91/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  55\n",
            "[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  55\n",
            "[ 93/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  55\n",
            "[ 94/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  57\n",
            "[ 95/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  57\n",
            "[ 96/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  57\n",
            "[ 97/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  58\n",
            "[ 98/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  58\n",
            "[ 99/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  61\n",
            "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  61\n",
            "[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
            "[102/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
            "[103/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
            "[104/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  61\n",
            "[105/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  62\n",
            "[106/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  66\n",
            "[107/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  66\n",
            "[108/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  67\n",
            "[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  67\n",
            "[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  67\n",
            "[111/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  67\n",
            "[112/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  67\n",
            "[113/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  67\n",
            "[114/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  67\n",
            "[115/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  68\n",
            "[116/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  71\n",
            "[117/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  71\n",
            "[118/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  71\n",
            "[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  71\n",
            "[120/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  71\n",
            "[121/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  71\n",
            "[122/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  71\n",
            "[123/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  71\n",
            "[124/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  72\n",
            "[125/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  73\n",
            "[126/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  73\n",
            "[127/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  73\n",
            "[128/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  73\n",
            "[129/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  74\n",
            "[130/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  77\n",
            "[131/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  77\n",
            "[132/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  77\n",
            "[133/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  78\n",
            "[134/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  78\n",
            "[135/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  78\n",
            "[136/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  78\n",
            "[137/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  78\n",
            "[138/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  78\n",
            "[139/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  79\n",
            "[140/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  81\n",
            "[141/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  81\n",
            "[142/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  81\n",
            "[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  82\n",
            "[144/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  83\n",
            "[145/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  83\n",
            "[146/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  83\n",
            "[147/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  83\n",
            "[148/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  83\n",
            "[149/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  83\n",
            "[150/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  85\n",
            "[151/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  88\n",
            "[152/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  88\n",
            "[153/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  89\n",
            "[154/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  89\n",
            "[155/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  89\n",
            "[156/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  92\n",
            "[157/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  92\n",
            "[158/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  93\n",
            "[159/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  93\n",
            "[160/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  93\n",
            "[161/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  94\n",
            "[162/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  94\n",
            "[163/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  94\n",
            "[164/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  94\n",
            "[165/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type F16  | T+  94\n",
            "[166/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type F16  | T+  94\n",
            "[167/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  94\n",
            "[168/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+  95\n",
            "[169/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+  96\n",
            "[170/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+  98\n",
            "[171/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  98\n",
            "[172/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+ 100\n",
            "[173/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 100\n",
            "[174/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 100\n",
            "[175/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 100\n",
            "[176/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 100\n",
            "[177/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 100\n",
            "[178/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 101\n",
            "[179/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 101\n",
            "[180/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+ 101\n",
            "[181/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+ 101\n",
            "[182/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 101\n",
            "[183/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 102\n",
            "[184/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 105\n",
            "[185/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 105\n",
            "[186/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 105\n",
            "[187/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 106\n",
            "[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 106\n",
            "[189/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+ 106\n",
            "[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+ 106\n",
            "[191/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 106\n",
            "[192/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 107\n",
            "[193/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 107\n",
            "[194/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 108\n",
            "[195/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 108\n",
            "[196/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 109\n",
            "[197/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 112\n",
            "[198/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+ 112\n",
            "[199/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+ 112\n",
            "[200/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 112\n",
            "[201/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 112\n",
            "[202/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 112\n",
            "[203/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 112\n",
            "[204/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 112\n",
            "[205/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 113\n",
            "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 114\n",
            "[207/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+ 114\n",
            "[208/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+ 115\n",
            "[209/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 115\n",
            "[210/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 115\n",
            "[211/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 117\n",
            "[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 117\n",
            "[213/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 117\n",
            "[214/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 119\n",
            "[215/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 120\n",
            "[216/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+ 122\n",
            "[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+ 122\n",
            "[218/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 122\n",
            "[219/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 122\n",
            "[220/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 122\n",
            "[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 122\n",
            "[222/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 123\n",
            "[223/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 127\n",
            "[224/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 127\n",
            "[225/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+ 128\n",
            "[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+ 128\n",
            "[227/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 128\n",
            "[228/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 128\n",
            "[229/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 128\n",
            "[230/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 128\n",
            "[231/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 128\n",
            "[232/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 130\n",
            "[233/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 132\n",
            "[234/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+ 132\n",
            "[235/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+ 132\n",
            "[236/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 132\n",
            "[237/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 132\n",
            "[238/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 132\n",
            "[239/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 133\n",
            "[240/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 133\n",
            "[241/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 135\n",
            "[242/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 136\n",
            "[243/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+ 137\n",
            "[244/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+ 137\n",
            "[245/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 137\n",
            "[246/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 137\n",
            "[247/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 137\n",
            "[248/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 137\n",
            "[249/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 137\n",
            "[250/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 142\n",
            "[251/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 142\n",
            "[252/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+ 143\n",
            "[253/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+ 143\n",
            "[254/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 143\n",
            "[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 143\n",
            "[256/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 147\n",
            "[257/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 147\n",
            "[258/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 147\n",
            "[259/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 147\n",
            "[260/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 148\n",
            "[261/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+ 152\n",
            "[262/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+ 152\n",
            "[263/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 152\n",
            "[264/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 152\n",
            "[265/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 152\n",
            "[266/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 152\n",
            "[267/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 152\n",
            "[268/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 153\n",
            "[269/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 153\n",
            "[270/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+ 153\n",
            "[271/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+ 155\n",
            "[272/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 155\n",
            "[273/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 155\n",
            "[274/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 155\n",
            "[275/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 155\n",
            "[276/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 158\n",
            "[277/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 158\n",
            "[278/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 158\n",
            "[279/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+ 159\n",
            "[280/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+ 159\n",
            "[281/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+ 159\n",
            "[282/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type F16  | T+ 159\n",
            "[283/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type F16  | T+ 159\n",
            "[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+ 159\n",
            "[285/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type F16  | T+ 160\n",
            "[286/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type F16  | T+ 164\n",
            "[287/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type F16  | T+ 164\n",
            "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+ 164\n",
            "[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+ 164\n",
            "[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+ 164\n",
            "[291/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+ 164\n",
            "Wrote models/openHermes-7b/ggml-model-f16.gguf\n",
            "Requirement already satisfied: sentencepiece in /usr/local/envs/venv/lib/python3.8/site-packages (0.1.99)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0musage: ./quantize [--help] [--allow-requantize] [--leave-output-tensor] [--pure] model-f32.gguf [model-quant.gguf] type [nthreads]\n",
            "\n",
            "  --allow-requantize: Allows requantizing tensors that have already been quantized. Warning: This can severely reduce quality compared to quantizing from 16bit or 32bit\n",
            "  --leave-output-tensor: Will leave output.weight un(re)quantized. Increases model size but may also increase quality, especially when requantizing\n",
            "  --pure: Disable k-quant mixtures and quantize all tensors to the same type\n",
            "\n",
            "Allowed quantization types:\n",
            "   2  or  Q4_0   :  3.56G, +0.2166 ppl @ LLaMA-v1-7B\n",
            "   3  or  Q4_1   :  3.90G, +0.1585 ppl @ LLaMA-v1-7B\n",
            "   8  or  Q5_0   :  4.33G, +0.0683 ppl @ LLaMA-v1-7B\n",
            "   9  or  Q5_1   :  4.70G, +0.0349 ppl @ LLaMA-v1-7B\n",
            "  10  or  Q2_K   :  2.63G, +0.6717 ppl @ LLaMA-v1-7B\n",
            "  12  or  Q3_K   : alias for Q3_K_M\n",
            "  11  or  Q3_K_S :  2.75G, +0.5551 ppl @ LLaMA-v1-7B\n",
            "  12  or  Q3_K_M :  3.07G, +0.2496 ppl @ LLaMA-v1-7B\n",
            "  13  or  Q3_K_L :  3.35G, +0.1764 ppl @ LLaMA-v1-7B\n",
            "  15  or  Q4_K   : alias for Q4_K_M\n",
            "  14  or  Q4_K_S :  3.59G, +0.0992 ppl @ LLaMA-v1-7B\n",
            "  15  or  Q4_K_M :  3.80G, +0.0532 ppl @ LLaMA-v1-7B\n",
            "  17  or  Q5_K   : alias for Q5_K_M\n",
            "  16  or  Q5_K_S :  4.33G, +0.0400 ppl @ LLaMA-v1-7B\n",
            "  17  or  Q5_K_M :  4.45G, +0.0122 ppl @ LLaMA-v1-7B\n",
            "  18  or  Q6_K   :  5.15G, -0.0008 ppl @ LLaMA-v1-7B\n",
            "   7  or  Q8_0   :  6.70G, +0.0004 ppl @ LLaMA-v1-7B\n",
            "   1  or  F16    : 13.00G              @ 7B\n",
            "   0  or  F32    : 26.00G              @ 7B\n",
            "          COPY   : only copy tensors, no quantizing\n"
          ]
        }
      ],
      "source": [
        "!rm -rf *\n",
        "!git lfs install\n",
        "!sudo apt update\n",
        "!sudo apt install -y libimagequant-dev\n",
        "!sudo apt install -y make\n",
        "!sudo apt install -y cmake\n",
        "!git clone https://github.com/CesarChaMal/llamaCpp.git\n",
        "!mv llamaCpp/* .\n",
        "!rm -rf llamaCpp/\n",
        "#!git clone https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B openHermes-7b-v2.5\n",
        "#!mv openHermes-7b-v2.5 models/\n",
        "!git clone https://huggingface.co/teknium/OpenHermes-7B openHermes-7b\n",
        "!mv openHermes-7b models/\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!source /usr/local/etc/profile.d/conda.sh && conda init bash\n",
        "!source ~/.bashrc\n",
        "!conda create --name venv python=3.8 -y\n",
        "!source /usr/local/etc/profile.d/conda.sh && conda activate venv\n",
        "!/usr/local/envs/venv/bin/python --version\n",
        "!python -c \"import os; print('Current Working Directory:', os.getcwd())\"\n",
        "!/usr/local/envs/venv/bin/python -c \"import os; print('Current Working Directory:', os.getcwd())\"\n",
        "!/usr/local/envs/venv/bin/python -m pip install -r /content/requirements.txt\n",
        "!cmake ..\n",
        "!make\n",
        "#!/usr/local/envs/venv/bin/python /content/convert.py ./models/openHermes-7b-v2.5 --outfile ./models/openHermes-7b-v2.5/ggml-model-f16.gguf --outtype f16\n",
        "!/usr/local/envs/venv/bin/python convert.py ./models/openHermes-7b --outfile ./models/openHermes-7b/ggml-model-f16.gguf --outtype f16\n",
        "\n",
        "!/usr/local/envs/venv/bin/python -m pip install sentencepiece\n",
        "\n",
        "#./quantize models/openHermes-7b-v2..5/ggml-model-f16.gguf models/openHermes-7b-v2..5/ggml-model-q8_0.gguf q8_0\n",
        "!./quantize models/openHermes-7b/ggml-model-f16.gguf models/openHermes-7b/ggml-model-q8_0.gguf q8_0\n",
        "\n",
        "#./quantize models/openHermes-7b-v2..5/ggml-model-f16.gguf models/openHermes-7b-v2..5/ggml-model-q4_k.gguf q4_k\n",
        "!./quantize models/openHermes-7b/ggml-model-f16.gguf models/openHermes-7b/ggml-model-q4_k.gguf q4_k\n",
        "\n",
        "#!./batched-bench ./models/openhermes-7b-v2.5/ggml-model-f16.gguf 4096 0 99 0 2048 128,512 1,2,3,4\n",
        "!./batched-bench ./models/openhermes-7b/ggml-model-f16.gguf 4096 0 99 0 2048 128,512 1,2,3,4\n",
        "\n",
        "#!./server -m models/openhermes-7b-v2.5/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512\n",
        "!./server -m models/openhermes-7b/ggml-model-q4_k.gguf --port 8888 --host 0.0.0.0 --ctx-size 10240 --parallel 4 -ngl 99 -n 512\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}